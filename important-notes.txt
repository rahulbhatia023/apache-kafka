Approaches to send a message to kafka
=====================================

Fire-and-forget Producer
------------------------
Send and forget is the simplest approach. In this method, we send a message to the broker and donâ€™t care if it was successfully received or not.
You might be wondering that is this a right approach? Where to use that method?
Well, Kafka is a distributed system. It comes with inbuilt fault tolerance feature. That makes Kafka a highly available system.
So most of the time, your message will reach to the broker. We also know that producer will automatically retry in case of recoverable error.
So, the probability of losing your messages is thin.
There are many use cases where you are dealing with huge volumes and losing a small portion of records is not critical.
For example, if you are counting hits on a video, or collecting a twitter feed for sentiment analysis. In such use cases,
even if you lose 2-3% of your tweets, it may not be a problem.
But, it is important to understand that in fire and forget approach, you may lose some messages.
So, don't use this method when you can't afford to lose your messages.

Synchronous Producer
--------------------
In this approach, we send a message and wait until we get a response.
In the case of a success, we get a RecordMetadata object, and in the event of failure, we get an exception.
Most of the time, we don't care about the success and the RecordMetadata that we receive.
We only care about exception because we want to log errors for later analysis and appropriate action.
You can adopt this method if your messages are critical and you can't afford to lose anything.
But it is important to notice that synchronous approach will slow you down.
It will limit your throughput because you are waiting for every message to get acknowledged.
You are sending one message and waiting for success, then you send your next message, and again wait for success.
Each message will take some time to be delivered over the network.
So, after every message, you will be waiting for a network delay.
And the most interesting thing is that, you may not be doing anything in case of success.
You care only failures, if it fails, you may want to take some action.
The fire and forget approach was on one extreme, and the synchronous approach is at another extreme.
I mean, in one approach, you don't care at all, and in another method, you wait for every single message.
So, there is a third approach which takes the middle path.

Asynchronous Producer
---------------------
In this method, we send a message and provide a call back function to receive acknowledgment.
We don't wait for success and failure. The producer will callback our function with RecordMetadata and an exception object.
So, if you just care about exceptions, you simply look at the exception, if it is null, don't do anything.
If the exception is not null, then you know it failed, so you can record the message details for later analysis.
In this approach, you keep sending messages as fast as you can without waiting for responses,
and handle failures later as they come using a callback function.
The asynchronous method appears to provide you a throughput that is as good as fire and forget approach.
But there is a catch. You have a limit of in-flight messages.
This limit is controlled by a configuration parameter max.in.flight.requests.per.connection.
This parameter controls that how many messages you can send to the server without receiving a response. The default value is 5.
You can increase this value, but there are other considerations.
Asynchronous send gives you a better throughput compared to synchronous, but the max.in.flight.requests.per.connection limits it.

There is a side effect of asynchronous send. Let's assume you send 10 requests for same partition.
5 of them were sent as the first batch and failed. Remaining five goes as a second batch and succeed.
Now the producer will retry the first batch, and if it is successful, you lost your order.
That's a significant side effect of asynchronous send. So, be careful if the order of delivery is critical for you.
If you are looking for an ordered delivery of your messages, you have following two options.

Use synchronous send.
set max.in.flight.requests.per.connection to 1

Both options have the same result. The order is critical for some use cases, especially transactional systems, for example banks and inventory.
If you are working on that kind of use case, set max.in.flight.requests.per.connection to 1.


Problems with Kafka Custom Serializer/Deserializer
==================================================
The problem with this approach is managing future changes in the schema.
Suppose you implemented this serializer and deserializer, and your system is functional for few months.
After few months, you have a requirement to add another field in the supplier object.
If you modify your Supplier object, you must change your serializer and deserializer, may be the producer and consumer as well.
But the problem doesn't end there.
After making new changes, you can't read your older messages because you changed the format and modified your code to read the new format.
That's where generic serializers like Avro will be helpful.


Kafka Producer Configs
======================

acks
----
The acks configuration is to configure acknowledgments.
When producers send a message to Kafka broker, they get a response back from the broker.
The response is a RecordMetaData object or an exception.
This parameter acks, it can take three values: 0, 1, and all.

acks=0 : the producer will not wait for the response. It will send the message over the network and forget.
There are side effects of acks being 0:
Possible loss of messages
No Retries

acks=1: the producer will wait for the response. However, the response is sent by the leader.
In this case, the leader will respond after recording the message in its local storage.
If the leader is down and message delivery fails, the producer can retry after few milliseconds.
This option appears to be a safe choice. You still cannot guarantee that you will not lose your message.
We are not sure that it is replicated.
If the leader breaks before replica could make a copy, the message will be lost.
Surprisingly, in such scenario, the messages can be lost even after successful acknowledgment.
If you want to achieve 100% reliability, it is necessary that all replicas in the ISR list should make a copy successfully
before the leader sends an acknowledgment.

acks=all: If we set acks parameter to all, the leader will acknowledge only after it receives an acknowledgment from all of the live replicas.
This option gives you the highest reliability but costs you the highest latency.

retries
-------
It defines how many times the producer will retry after getting an error. The default value is 0.
There is another parameter 'retry.backoff.ms' that controls the time between two retries. The default value for this parameter is 100 milliseconds.


Kafka Consumer Groups
=====================

If your producers are pushing data to the topic at a moderate speed, a single consumer may be enough to read and process that data.
However, if you want to scale up your system and read data from Kafka in parallel, you need multiple consumers reading your topic in parallel.
Many applications may have a clear need for multiple producers pushing data to a topic at one end and multiple consumers reading and processing data on the other end.
There is no complexity at the producer side. It is as simple as executing another instance of a producer.
There is no coordination or sharing of information needed among producers.
But on the consumer side, we have various considerations.

When I talk about parallel reading, I am speaking about one single application consuming data in parallel.
It is not about multiple applications reading same Kafka topic in parallel.
So, the question is, how to implement parallel reads in a single application.
We can do that by creating a group and starting multiple consumers in the same group.

If we have multiple consumers reading data in parallel from the same topic, don't you think that all of them can read the same message?
The answer is no. Only one consumer owns a partition at any point in time.

Let's take an example to understand this.
We have one topic, and there are four partitions. So, if we have only one consumer in a group, it reads from all four partitions.
If you have two, each of them reads two partitions.
If you have three, the arrangement may be something like a single consumer reading two partitions and others own a single partition each.
So, the fundamental concept is that the consumers do not share a partition. There is no way we can read the same message more than once.
However, this solution also brings a limitation. The number of partitions on a topic is the upper limit of consumers you can have in a group.
So, in our example, if you have five consumers, one of them reads nothing.
Kafka won't complain that you have four partitions, but you are starting five consumers. Simply, the fifth consumer will have nothing to read.

How does a consumer enter and exit into a group?
-----------------------------------------------
Question is, how Kafka handles it?
When a consumer joins a group, how is a partition assigned to it?
Moreover, what happens to the partition when a consumer leaves the group? Who manages all of this?

Kafka Group Coordinator
-----------------------
A group coordinator oversees all of this. So, one of the Kafka broker gets elected as a Group Coordinator.
When a consumer wants to join a group, it sends a request to the coordinator. The first consumer to participate in a group becomes a leader.
All other consumers joining later becomes the members of the group.
So, we have two actors, A coordinator, and a group leader. The coordinator is responsible for managing a list of group members.
So, every time a new member joins the group, or an existing member leaves the group, the coordinator modifies the list.
On an event of membership change, the coordinator realizes that it is time to rebalance the partition assignment.
Because you may have a new member, and you need to assign it some partitions, or a member left, and you need to reassign those partitions to someone else.
So, every time the list is modified, the coordinator initiates a rebalance activity.

Kafka Group Leader
------------------
The group leader is responsible for executing rebalance activity.
The group leader will take a list of current members, assign partitions to them and send it back to the coordinator.
The Coordinator then communicates back to the members about their new partitions.
The important thing to note here is, during the rebalance activity, none of the consumers are allowed to read any message.


Kafka Consumer: The Poll Method
===============================
The poll function is pretty powerful and takes care of a lot of things.
It handles all the coordination, partition rebalances, and heart beat for group coordinator.
When you call to poll for the first time from a consumer, it finds a group coordinator, joins the group, receives partition assignment
and fetches some records from those partitions.
Every time you call to poll, it will send a heartbeat to group coordinator.
So, it is necessary that whatever you do in a consumer, it should be quick and efficient.
If you don't poll for a while, the coordinator may assume that the consumer is dead and trigger a partition rebalance.